{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure NLTK punkt tokenizer is downloaded\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
    "    return embeddings_index\n",
    "\n",
    "# Use the path where you extracted GloVe vectors\n",
    "glove_file = './glove.6B/glove.6B.300d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load FLAN-T5 model (pre-downloaded)\n",
    "custom_model_dir = \"./flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(custom_model_dir, local_files_only=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(custom_model_dir, local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate an answer or summary using FLAN-T5\n",
    "def generate_answer_with_flan(query, context):\n",
    "    inputs = tokenizer(f\"Context: {context} Question: {query}\", return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    outputs = model.generate(**inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "# Function to get the GloVe vector of a word\n",
    "def get_glove_vector(word, glove_embeddings, embedding_dim=300):\n",
    "    return glove_embeddings.get(word, np.zeros(embedding_dim))\n",
    "\n",
    "# Convert a sentence into a vector by averaging word vectors\n",
    "def sentence_to_glove_vector(sentence, glove_embeddings, embedding_dim=300):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word_vectors = [torch.tensor(get_glove_vector(word, glove_embeddings, embedding_dim), device=device) for word in words if word in glove_embeddings]\n",
    "    \n",
    "    if not word_vectors:\n",
    "        return torch.zeros(embedding_dim, device=device)\n",
    "    \n",
    "    return torch.mean(torch.stack(word_vectors), dim=0)\n",
    "\n",
    "# Convert the entire document to GloVe vectors (sentence-level)\n",
    "def document_to_glove_vectors(document_sentences, glove_embeddings, embedding_dim=300):\n",
    "    vectors = []\n",
    "    for sentence in document_sentences:\n",
    "        vector = sentence_to_glove_vector(sentence, glove_embeddings, embedding_dim)\n",
    "        if vector.norm() > 0:\n",
    "            vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "# Function to find the most relevant sentences from the document based on the query\n",
    "def find_relevant_context(query, document_vectors, document_sentences, glove_embeddings):\n",
    "    query_vector = sentence_to_glove_vector(query, glove_embeddings)\n",
    "    \n",
    "    if len(document_vectors) == 0:\n",
    "        return \"No relevant context found.\"\n",
    "    \n",
    "    document_vectors_tensor = torch.stack(document_vectors).to(device)\n",
    "    similarities = torch.nn.functional.cosine_similarity(query_vector.unsqueeze(0), document_vectors_tensor)\n",
    "    \n",
    "    top_indices = torch.topk(similarities, k=5).indices\n",
    "    top_context = \" \".join([document_sentences[i] for i in top_indices])\n",
    "    \n",
    "    return top_context\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Main function for chatbot\n",
    "def chatbot_with_glove_and_flan(pdf_path, glove_embeddings):\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "    if not document_text.strip():\n",
    "        print(\"Extracted text is empty. Please check the PDF file.\")\n",
    "        return\n",
    "    \n",
    "    document_sentences = sent_tokenize(document_text.replace(\"\\n\",\" \"))\n",
    "    if not document_sentences:\n",
    "        print(\"No sentences found in the document.\")\n",
    "        return\n",
    "    \n",
    "    document_vectors = document_to_glove_vectors(document_sentences, glove_embeddings)\n",
    "    \n",
    "    if len(document_vectors) == 0:\n",
    "        print(\"No valid document vectors found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Chatbot is ready! Ask your questions.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() in ['exit', 'quit', 'end']:\n",
    "            break\n",
    "        \n",
    "        relevant_context = find_relevant_context(query, document_vectors, document_sentences, glove_embeddings)\n",
    "        answer = generate_answer_with_flan(query, relevant_context)\n",
    "        print(f\"Chatbot: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_20720\\2131478853.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  pdf_path = \"D:\\sem-7\\Blockchain\\Theory\\Introduction to Blockchain.pdf\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Ask your questions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: What is the name of the project?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "pdf_path = \"D:\\sem-7\\Blockchain\\Theory\\Introduction to Blockchain.pdf\"\n",
    "chatbot_with_glove_and_flan(pdf_path, glove_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymupdf-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
